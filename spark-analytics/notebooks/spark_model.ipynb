{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please verify that you have pyspark and the required java versions installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==3.0.0\n",
      "  Using cached pyspark-3.0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: py4j==0.10.9 in c:\\users\\ethan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyspark==3.0.0) (0.10.9)\n",
      "Installing collected packages: pyspark\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -yspark (c:\\users\\ethan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yspark (c:\\users\\ethan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 32] The process cannot access the file because it is being used by another process: 'c:\\\\Users\\\\ethan\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\Lib\\\\site-packages\\\\pyspark\\\\jars\\\\HikariCP-2.5.1.jar'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark==3.0.0 && java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import pickle\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"pyspark-shell\"\n",
    "\n",
    "data_path = pathlib.Path().cwd().parent / \"spark-analytics\" / \"1m_health_events_dataset.csv\"\n",
    "model_path = pathlib.Path().cwd().parent / \"spark-analytics\" / \"pyspark_models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark KMeans Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import month\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"HealthRiskPrediction\").getOrCreate()\n",
    "\n",
    "data = spark.read.csv(\"1m_health_events_dataset.csv\", header=True, inferSchema=True)\n",
    "data = data.withColumn(\"Timestamp\", data[\"Timestamp\"].cast(\"timestamp\"))\n",
    "data = data.withColumn(\"Month\", month(data[\"Timestamp\"]))\n",
    "\n",
    "# do a string indexing on categorical columns\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\") for column in [\"EventType\", \"Location\", \"Severity\"]]\n",
    "\n",
    "# create a feature vector for risk prediction\n",
    "riskFeatures = [\"EventType_index\", \"Location_index\", \"Severity_index\", \"Month\"]\n",
    "riskAssembler = VectorAssembler(inputCols=riskFeatures, outputCol=\"riskFeatures\")\n",
    "\n",
    "# create a feature vector for anomaly detection\n",
    "anomalyFeatures = [\"EventType_index\", \"Location_index\", \"Severity_index\"]\n",
    "anomalyAssembler = VectorAssembler(inputCols=anomalyFeatures, outputCol=\"anomalyFeatures\")\n",
    "\n",
    "(trainingData, testData) = data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation & Evaluation #1\n",
    "KMeans Clustering - Risk prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risk Prediction Accuracy: 0.9474703434020741\n"
     ]
    }
   ],
   "source": [
    "# create a MLP classifier for risk prediction\n",
    "riskLayers = [len(riskFeatures), 10, 5, 2]  # Input features, hidden units, output classes\n",
    "riskMLP = MultilayerPerceptronClassifier(labelCol=\"Is_Anomaly\", featuresCol=\"riskFeatures\", layers=riskLayers, seed=42)\n",
    "\n",
    "# create a pipeline for risk prediction\n",
    "riskPipeline = Pipeline(stages=indexers + [riskAssembler, riskMLP])\n",
    "\n",
    "riskModel = riskPipeline.fit(trainingData)\n",
    "riskPredictions = riskModel.transform(testData)\n",
    "\n",
    "# evaluate the risk prediction model\n",
    "riskEvaluator = BinaryClassificationEvaluator(labelCol=\"Is_Anomaly\")\n",
    "riskAccuracy = riskEvaluator.evaluate(riskPredictions)\n",
    "print(\"Risk Prediction Accuracy:\", riskAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation & Evaluation #2\n",
    "KMeans Clustering - Anomaly detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Detection Score: 0.5764826823551313\n"
     ]
    }
   ],
   "source": [
    "# create a KMeans model for anomaly detection\n",
    "kmeans = KMeans(featuresCol=\"anomalyFeatures\", k=2, seed=42)\n",
    "\n",
    "anomalyPipeline = Pipeline(stages=indexers + [anomalyAssembler, kmeans])\n",
    "anomalyModel = anomalyPipeline.fit(trainingData)\n",
    "\n",
    "anomalyPredictions = anomalyModel.transform(testData)\n",
    "\n",
    "# evaluate the anomaly detection model\n",
    "anomalyEvaluator = ClusteringEvaluator(featuresCol=\"anomalyFeatures\")\n",
    "anomalyScore = anomalyEvaluator.evaluate(anomalyPredictions)\n",
    "print(\"Anomaly Detection Score:\", anomalyScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Saving\n",
    "Currently does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'pass'\n"
     ]
    }
   ],
   "source": [
    "%%script pass\n",
    "\n",
    "# save our models, and feel free to change to your desired local path!\n",
    "riskModel.write().overwrite().save(f'{model_path / \"risk_model\"}')\n",
    "anomalyModel.write().overwrite().save(f'{model_path / \"anomaly_model\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, unix_timestamp, datediff, current_timestamp\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HealthRiskPrediction\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# load the CSV data\n",
    "data = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(f'{data_path}')\n",
    "\n",
    "# preprocess the data\n",
    "data = data.withColumn(\"days_since_event\", datediff(current_timestamp(), col(\"Timestamp\")))\n",
    "data = data.withColumn(\"Timestamp\", unix_timestamp(\"Timestamp\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# encode categorical variables\n",
    "event_indexer = StringIndexer(inputCol=\"EventType\", outputCol=\"EventTypeIndex\")\n",
    "location_indexer = StringIndexer(inputCol=\"Location\", outputCol=\"LocationIndex\")\n",
    "severity_indexer = StringIndexer(inputCol=\"Severity\", outputCol=\"SeverityIndex\")\n",
    "\n",
    "# one-hot encode categorical variables\n",
    "event_encoder = OneHotEncoder(inputCol=\"EventTypeIndex\", outputCol=\"EventTypeVec\")\n",
    "location_encoder = OneHotEncoder(inputCol=\"LocationIndex\", outputCol=\"LocationVec\")\n",
    "severity_encoder = OneHotEncoder(inputCol=\"SeverityIndex\", outputCol=\"SeverityVec\")\n",
    "\n",
    "# create feature vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"Timestamp\", \"days_since_event\", \"EventTypeVec\", \"LocationVec\", \"SeverityVec\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# split the data into training and test sets\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Random Forest classifier\n",
    "rf = RandomForestClassifier(labelCol=\"Is_Anomaly\", featuresCol=\"features\", numTrees=100)\n",
    "\n",
    "# create a pipeline\n",
    "pipeline = Pipeline(stages=[\n",
    "    event_indexer, location_indexer, severity_indexer,\n",
    "    event_encoder, location_encoder, severity_encoder,\n",
    "    assembler, rf\n",
    "])\n",
    "\n",
    "# set up parameter grid for hyperparameter tuning\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
    "    .addGrid(rf.maxBins, [16, 32, 64]) \\\n",
    "    .build()\n",
    "\n",
    "# create a binary classification evaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Is_Anomaly\")\n",
    "\n",
    "# create a cross-validator\n",
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=3, parallelism=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.95055473\n"
     ]
    }
   ],
   "source": [
    "model = cv.fit(train_data)\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# evaluate our model\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Model accuracy: {accuracy:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'pass'\n"
     ]
    }
   ],
   "source": [
    "%%script pass\n",
    "\n",
    "model.write().overwrite().save(f'{model_path}/randomforest_cv_model_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, unix_timestamp, datediff, current_timestamp\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HealthRiskPrediction\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# load the CSV data\n",
    "data = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(f'{data_path}')\n",
    "\n",
    "# sample a subset of the data for training and testing\n",
    "sampled_data = data.sample(fraction=1.0, seed=42)\n",
    "\n",
    "# preprocess the data\n",
    "sampled_data = sampled_data.withColumn(\"days_since_event\", datediff(current_timestamp(), col(\"Timestamp\")))\n",
    "sampled_data = sampled_data.withColumn(\"Timestamp\", unix_timestamp(\"Timestamp\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# encode categorical variables\n",
    "event_indexer = StringIndexer(inputCol=\"EventType\", outputCol=\"EventTypeIndex\")\n",
    "location_indexer = StringIndexer(inputCol=\"Location\", outputCol=\"LocationIndex\")\n",
    "severity_indexer = StringIndexer(inputCol=\"Severity\", outputCol=\"SeverityIndex\")\n",
    "\n",
    "# one-hot encode categorical variables\n",
    "event_encoder = OneHotEncoder(inputCol=\"EventTypeIndex\", outputCol=\"EventTypeVec\")\n",
    "location_encoder = OneHotEncoder(inputCol=\"LocationIndex\", outputCol=\"LocationVec\")\n",
    "severity_encoder = OneHotEncoder(inputCol=\"SeverityIndex\", outputCol=\"SeverityVec\")\n",
    "\n",
    "# create feature vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"Timestamp\", \"days_since_event\", \"EventTypeVec\", \"LocationVec\", \"SeverityVec\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# split the data into training and test sets\n",
    "train_data, test_data = sampled_data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Random Forest classifier\n",
    "rf = RandomForestClassifier(labelCol=\"Is_Anomaly\", featuresCol=\"features\", numTrees=50)\n",
    "\n",
    "# create a pipeline\n",
    "pipeline = Pipeline(stages=[\n",
    "    event_indexer, location_indexer, severity_indexer,\n",
    "    event_encoder, location_encoder, severity_encoder,\n",
    "    assembler, rf\n",
    "])\n",
    "\n",
    "# set up parameter grid for hyperparameter tuning\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.maxDepth, [5, 10]) \\\n",
    "    .addGrid(rf.maxBins, [16, 32]) \\\n",
    "    .build()\n",
    "\n",
    "# create a binary classification evaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Is_Anomaly\")\n",
    "\n",
    "# create a cross-validator with fewer folds\n",
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=2, parallelism=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.95087880\n"
     ]
    }
   ],
   "source": [
    "model = cv.fit(train_data)\n",
    "\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# evaluate our model\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Model accuracy: {accuracy:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, unix_timestamp, datediff, current_timestamp, hour, dayofweek, month\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, MinMaxScaler\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HealthRiskPrediction\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# load the CSV data\n",
    "data = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(f'{data_path}')\n",
    "\n",
    "# sample a subset of the data for training and testing\n",
    "sampled_data = data.sample(fraction=1.0, seed=42)\n",
    "\n",
    "# preprocess the data\n",
    "sampled_data = sampled_data.withColumn(\"days_since_event\", datediff(current_timestamp(), col(\"Timestamp\")))\n",
    "sampled_data = sampled_data.withColumn(\"hour\", hour(col(\"Timestamp\")))\n",
    "sampled_data = sampled_data.withColumn(\"day_of_week\", dayofweek(col(\"Timestamp\")))\n",
    "sampled_data = sampled_data.withColumn(\"month\", month(col(\"Timestamp\")))\n",
    "sampled_data = sampled_data.withColumn(\"Timestamp\", unix_timestamp(\"Timestamp\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# encode categorical variables\n",
    "event_indexer = StringIndexer(inputCol=\"EventType\", outputCol=\"EventTypeIndex\")\n",
    "location_indexer = StringIndexer(inputCol=\"Location\", outputCol=\"LocationIndex\")\n",
    "severity_indexer = StringIndexer(inputCol=\"Severity\", outputCol=\"SeverityIndex\")\n",
    "\n",
    "# one-hot encode categorical variables\n",
    "event_encoder = OneHotEncoder(inputCol=\"EventTypeIndex\", outputCol=\"EventTypeVec\")\n",
    "location_encoder = OneHotEncoder(inputCol=\"LocationIndex\", outputCol=\"LocationVec\")\n",
    "severity_encoder = OneHotEncoder(inputCol=\"SeverityIndex\", outputCol=\"SeverityVec\")\n",
    "\n",
    "# create feature vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"Timestamp\", \"days_since_event\", \"hour\", \"day_of_week\", \"month\", \"EventTypeVec\", \"LocationVec\", \"SeverityVec\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# scale features\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# split the data into training and test sets\n",
    "train_data, test_data = sampled_data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create classifiers\n",
    "rf = RandomForestClassifier(labelCol=\"Is_Anomaly\", featuresCol=\"scaledFeatures\", numTrees=100)\n",
    "gbt = GBTClassifier(labelCol=\"Is_Anomaly\", featuresCol=\"scaledFeatures\", maxIter=50)\n",
    "\n",
    "# create a pipeline\n",
    "pipeline = Pipeline(stages=[\n",
    "    event_indexer, location_indexer, severity_indexer,\n",
    "    event_encoder, location_encoder, severity_encoder,\n",
    "    assembler, scaler, rf\n",
    "])\n",
    "\n",
    "# set up parameter grid for hyperparameter tuning\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
    "    .addGrid(rf.maxBins, [16, 32, 64]) \\\n",
    "    .addGrid(rf.numTrees, [50, 100, 200]) \\\n",
    "    .build()\n",
    "\n",
    "# create evaluators\n",
    "binary_evaluator = BinaryClassificationEvaluator(labelCol=\"Is_Anomaly\")\n",
    "multiclass_evaluator = MulticlassClassificationEvaluator(labelCol=\"Is_Anomaly\", metricName=\"accuracy\")\n",
    "\n",
    "# create a cross-validator\n",
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=param_grid, evaluator=binary_evaluator, numFolds=3, parallelism=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cv.fit(train_data)\n",
    "\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# evaluate our model\n",
    "binary_metrics = binary_evaluator.evaluate(predictions)\n",
    "multiclass_accuracy = multiclass_evaluator.evaluate(predictions)\n",
    "print(f\"Binary Classification Metrics: {binary_metrics:.4f}\")\n",
    "print(f\"Multiclass Classification Accuracy: {multiclass_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## May 4th\n",
    "\n",
    "Enhanced model training pipeline by adding StandardScaler and GBTClassifier options, and implemented model versioning for zero-downtime deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, unix_timestamp, datediff, current_timestamp, hour, dayofweek, month\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# constants\n",
    "DATA_PATH = \"../data/1m_health_events_dataset.csv\"\n",
    "MODEL_NAME = \"health_risk_model_notebook\"\n",
    "MODEL_DIR = \"../models\"\n",
    "\n",
    "# create SparkSession\n",
    "spark = SparkSession.builder.appName(\"HealthRiskPrediction\").getOrCreate()\n",
    "\n",
    "data = spark.read.csv(DATA_PATH, header=True, inferSchema=True)\n",
    "\n",
    "# preprocess the data\n",
    "preprocessed_data = data \\\n",
    "    .withColumn(\"days_since_event\", datediff(current_timestamp(), col(\"Timestamp\"))) \\\n",
    "    .withColumn(\"hour\", hour(col(\"Timestamp\"))) \\\n",
    "    .withColumn(\"day_of_week\", dayofweek(col(\"Timestamp\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"Timestamp\"))) \\\n",
    "    .withColumn(\"Timestamp\", unix_timestamp(\"Timestamp\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# split the data into training and testing sets\n",
    "train_data, test_data = preprocessed_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# create the pipeline\n",
    "event_indexer = StringIndexer(inputCol=\"EventType\", outputCol=\"EventTypeIndex\")\n",
    "location_indexer = StringIndexer(inputCol=\"Location\", outputCol=\"LocationIndex\")\n",
    "severity_indexer = StringIndexer(inputCol=\"Severity\", outputCol=\"SeverityIndex\")\n",
    "\n",
    "event_encoder = OneHotEncoder(inputCol=\"EventTypeIndex\", outputCol=\"EventTypeVec\")\n",
    "location_encoder = OneHotEncoder(inputCol=\"LocationIndex\", outputCol=\"LocationVec\")\n",
    "severity_encoder = OneHotEncoder(inputCol=\"SeverityIndex\", outputCol=\"SeverityVec\")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"Timestamp\", \"days_since_event\", \"hour\", \"day_of_week\", \"month\", \"EventTypeVec\", \"LocationVec\", \"SeverityVec\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "classifier = RandomForestClassifier(labelCol=\"Is_Anomaly\", featuresCol=\"scaledFeatures\", numTrees=100)\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    event_indexer, location_indexer, severity_indexer,\n",
    "    event_encoder, location_encoder, severity_encoder,\n",
    "    assembler, scaler, classifier\n",
    "])\n",
    "\n",
    "# train the pipeline\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(classifier.maxDepth, [5, 10, 15]) \\\n",
    "    .addGrid(classifier.maxBins, [16, 32, 64]) \\\n",
    "    .addGrid(classifier.impurity, [\"gini\", \"entropy\"]) \\\n",
    "    .build()\n",
    "\n",
    "binary_evaluator = BinaryClassificationEvaluator(labelCol=\"Is_Anomaly\")\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=param_grid, evaluator=binary_evaluator, numFolds=3, parallelism=4)\n",
    "model = cv.fit(train_data)\n",
    "\n",
    "# evaluate the model\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "binary_evaluator = BinaryClassificationEvaluator(labelCol=\"Is_Anomaly\")\n",
    "multiclass_evaluator = MulticlassClassificationEvaluator(labelCol=\"Is_Anomaly\", metricName=\"accuracy\")\n",
    "\n",
    "binary_metrics = binary_evaluator.evaluate(predictions)\n",
    "multiclass_accuracy = multiclass_evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Binary Classification Metrics: {binary_metrics:.4f}\")\n",
    "print(f\"Multiclass Classification Accuracy: {multiclass_accuracy:.4f}\")\n",
    "\n",
    "# save the model\n",
    "model_path = f\"{MODEL_DIR}/{MODEL_NAME}\"\n",
    "model.write().overwrite().save(model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
